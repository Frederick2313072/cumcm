#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AIéŸ³ä¹è´¨é‡è¯„ä»·ç³»ç»Ÿ - é—®é¢˜2è§£å†³æ–¹æ¡ˆ
åŸºäºæ•°å­¦å»ºæ¨¡çš„å¤šç»´åº¦AIéŸ³ä¹è¯„åˆ†ç³»ç»Ÿ

è®¾è®¡ç†å¿µï¼š
1. åŒºåˆ†AIå‚ä¸ç¨‹åº¦ï¼šè¾…åŠ©ç”Ÿæˆ vs ç›´æ¥ç”Ÿæˆ
2. è¯„ä¼°éŸ³ä¹è´¨é‡ï¼šæŠ€æœ¯è´¨é‡ã€è‰ºæœ¯è´¨é‡ã€å¬è§‰ä½“éªŒ
3. ç»¼åˆè¯„åˆ†ï¼šåŸºäºå±‚æ¬¡åˆ†ææ³•çš„åŠ æƒè¯„åˆ†æ¨¡å‹
4. æ•°å­¦åŸºç¡€ï¼šä¿¡å·å¤„ç†ã€ç»Ÿè®¡å­¦ã€éŸ³ä¹å£°å­¦ã€å¿ƒç†å£°å­¦
"""

import numpy as np
import librosa
import scipy.stats as stats
from scipy import signal
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, mean_squared_error
import warnings
import os
import glob
import pandas as pd
from typing import Dict, List, Tuple, Optional
import json

warnings.filterwarnings('ignore')

class AIMusicQualityEvaluator:
    """åŸºäºæ•°å­¦ç†è®ºçš„AIéŸ³ä¹è´¨é‡è¯„ä»·å™¨
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. AIå‚ä¸åº¦åˆ†æ - åŒºåˆ†äººå·¥è¾…åŠ©vså®Œå…¨AIç”Ÿæˆ
    2. å¤šç»´åº¦è´¨é‡è¯„ä¼° - æŠ€æœ¯ã€è‰ºæœ¯ã€å¬è§‰ä½“éªŒ
    3. ç»¼åˆè¯„åˆ†ç³»ç»Ÿ - 0-100åˆ†åˆ¶ï¼ŒåŸºäºAHPæƒé‡åˆ†é…
    4. å¯è§£é‡Šæ€§æŠ¥å‘Š - è¯¦ç»†çš„è¯„åˆ†ä¾æ®å’Œæ”¹è¿›å»ºè®®
    """
    
    def __init__(self, sr=22050):
        self.sr = sr
        self.scaler = StandardScaler()
        self.ai_type_classifier = None
        self.quality_regressor = None
        
        # AHPå±‚æ¬¡åˆ†ææ³•ç¡®å®šçš„æƒé‡ (åŸºäºä¸“å®¶è¯„åˆ†)
        self.dimension_weights = {
            'ai_involvement': 0.25,      # AIå‚ä¸åº¦æƒé‡
            'technical_quality': 0.35,   # æŠ€æœ¯è´¨é‡æƒé‡  
            'artistic_quality': 0.25,    # è‰ºæœ¯è´¨é‡æƒé‡
            'listening_experience': 0.15  # å¬è§‰ä½“éªŒæƒé‡
        }
        
        # è¯„åˆ†åŒºé—´å®šä¹‰
        self.score_ranges = {
            'human': (95, 100),                    # äººç±»åˆ›ä½œ
            'ai_assisted_high': (85, 95),         # é«˜è´¨é‡AIè¾…åŠ©
            'ai_assisted_low': (70, 85),          # ä½è´¨é‡AIè¾…åŠ©  
            'ai_direct_high': (60, 80),           # é«˜è´¨é‡AIç›´æ¥ç”Ÿæˆ
            'ai_direct_low': (20, 60),            # ä½è´¨é‡AIç›´æ¥ç”Ÿæˆ
            'ai_generated_poor': (0, 30)          # è´¨é‡æå·®çš„AIç”Ÿæˆ
        }
    
    def extract_ai_involvement_features(self, y: np.ndarray) -> Dict[str, float]:
        """æå–AIå‚ä¸åº¦ç›¸å…³ç‰¹å¾
        
        åŸºäºä»¥ä¸‹å‡è®¾ï¼š
        1. äººå·¥è°ƒæ ¡ä¼šç•™ä¸‹å¾®å¦™çš„ä¸è§„å¾‹æ€§ç—•è¿¹
        2. å®Œå…¨AIç”Ÿæˆå¾€å¾€è¿‡äºè§„å¾‹å’Œ"å®Œç¾"
        3. æŠ€æœ¯å¤æ‚åº¦åæ˜ åˆ›ä½œè€…çš„ä¸“ä¸šç¨‹åº¦
        """
        features = {}
        
        print("[AIå‚ä¸åº¦] åˆ†æäººå·¥è°ƒæ ¡ç—•è¿¹...")
        
        # 1. éŸ³é«˜å¾®è°ƒæ£€æµ‹ - äººå·¥è°ƒæ ¡ä¼šæœ‰ç»†å¾®çš„éŸ³é«˜åç§»
        try:
            pitches, magnitudes = librosa.piptrack(y=y, sr=self.sr, threshold=0.1)
            pitch_track = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                pitch = pitches[index, t] if pitches[index, t] > 0 else 0
                if pitch > 0:
                    pitch_track.append(pitch)
            
            if len(pitch_track) > 10:
                # éŸ³é«˜å˜åŒ–çš„å¾®è§‚ä¸è§„å¾‹æ€§ - äººå·¥è°ƒæ ¡ç‰¹å¾
                pitch_micro_variance = np.var(np.diff(pitch_track))
                features['pitch_micro_irregularity'] = min(pitch_micro_variance / 1000, 1.0)
                
                # éŸ³é«˜é‡åŒ–ç¨‹åº¦ - AIç”Ÿæˆå¾€å¾€è¿‡äºé‡åŒ–
                pitch_quantization = self._calculate_quantization_degree(pitch_track)
                features['pitch_quantization_degree'] = pitch_quantization
            else:
                features['pitch_micro_irregularity'] = 0.5
                features['pitch_quantization_degree'] = 0.5
        except:
            features['pitch_micro_irregularity'] = 0.5
            features['pitch_quantization_degree'] = 0.5
        
        # 2. æ—¶å€¼è°ƒæ•´æ£€æµ‹ - äººå·¥è°ƒæ ¡çš„èŠ‚æ‹å¾®è°ƒ
        print("[AIå‚ä¸åº¦] åˆ†æèŠ‚æ‹å¾®è°ƒç—•è¿¹...")
        try:
            tempo, beats = librosa.beat.beat_track(y=y, sr=self.sr)
            if len(beats) > 5:
                beat_intervals = np.diff(beats) / self.sr
                # èŠ‚æ‹é—´éš”çš„å¾®è§‚å˜åŒ– - äººç±»æ¼”å¥ç‰¹å¾
                beat_micro_variance = np.var(beat_intervals)
                features['beat_micro_irregularity'] = min(beat_micro_variance * 100, 1.0)
                
                # èŠ‚æ‹è§„å¾‹æ€§ - AIç”Ÿæˆå¾€å¾€è¿‡äºè§„å¾‹
                beat_regularity = 1.0 - np.std(beat_intervals) / np.mean(beat_intervals)
                features['beat_regularity'] = np.clip(beat_regularity, 0, 1)
            else:
                features['beat_micro_irregularity'] = 0.5
                features['beat_regularity'] = 0.5
        except:
            features['beat_micro_irregularity'] = 0.5
            features['beat_regularity'] = 0.5
        
        # 3. åˆ›ä½œå¤æ‚åº¦åˆ†æ
        print("[AIå‚ä¸åº¦] åˆ†æåˆ›ä½œå¤æ‚åº¦...")
        
        # å’Œå£°å¤æ‚åº¦ - åŸºäºç®€åŒ–çš„é¢‘è°±åˆ†æï¼ˆé¿å…è‰²åº¦è®¡ç®—ï¼‰
        try:
            # ä½¿ç”¨å®‰å…¨çš„é¢‘è°±åˆ†ææ›¿ä»£è‰²åº¦ç‰¹å¾
            y_short = y[:min(len(y), self.sr * 10)]  # é™åˆ¶åˆ°10ç§’
            if len(y_short) >= 2048:
                # ä½¿ç”¨numpy FFTè¿›è¡Œå®‰å…¨çš„é¢‘è°±åˆ†æ
                fft_result = np.fft.fft(y_short, n=4096)
                magnitude = np.abs(fft_result[:2048])
                
                # å°†é¢‘è°±åˆ†ä¸º12ä¸ªåŒºé—´æ¨¡æ‹Ÿå’Œå¼¦å¤æ‚åº¦
                freq_bins = len(magnitude)
                bin_size = max(1, freq_bins // 12)
                active_bins = 0
                
                for i in range(12):
                    start_idx = i * bin_size
                    end_idx = min((i + 1) * bin_size, freq_bins)
                    if start_idx < end_idx:
                        bin_energy = np.mean(magnitude[start_idx:end_idx])
                        if bin_energy > np.percentile(magnitude, 70):  # æ´»è·ƒé¢‘æ®µ
                            active_bins += 1
                
                chord_complexity = active_bins
            else:
                chord_complexity = 3  # é»˜è®¤å€¼
            
            features['harmonic_complexity'] = min(chord_complexity / 6, 1.0)
        except:
            features['harmonic_complexity'] = 0.5
        
        # æ—‹å¾‹å¤æ‚åº¦ - åŸºäºéŸ³é«˜å˜åŒ–ç†µ
        if len(pitch_track) > 10:
            pitch_changes = np.diff(pitch_track)
            pitch_change_entropy = stats.entropy(np.histogram(pitch_changes, bins=20)[0] + 1e-10)
            features['melodic_complexity'] = min(pitch_change_entropy / 3, 1.0)
        else:
            features['melodic_complexity'] = 0.5
        
        # 4. æŠ€æœ¯ç²¾ç»†åº¦è¯„ä¼°
        print("[AIå‚ä¸åº¦] è¯„ä¼°æŠ€æœ¯ç²¾ç»†åº¦...")
        
        # åŠ¨æ€æ§åˆ¶ç²¾ç»†åº¦ - åŸºäºRMSèƒ½é‡å˜åŒ–
        rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
        rms_gradient = np.gradient(rms)
        dynamic_control_fineness = np.std(rms_gradient)
        features['dynamic_control_fineness'] = min(dynamic_control_fineness * 10, 1.0)
        
        # é¢‘è°±é›•åˆ»ç²¾ç»†åº¦ - åŸºäºé¢‘è°±å˜åŒ–
        stft = librosa.stft(y)
        spectral_gradient = np.mean(np.abs(np.gradient(np.abs(stft), axis=1)))
        features['spectral_sculpting_fineness'] = min(spectral_gradient / 100, 1.0)
        
        print(f"[AIå‚ä¸åº¦] ç‰¹å¾æå–å®Œæˆï¼Œå…±{len(features)}ä¸ªç‰¹å¾")
        return features
    
    def extract_technical_quality_features(self, y: np.ndarray) -> Dict[str, float]:
        """æå–æŠ€æœ¯è´¨é‡ç‰¹å¾
        
        åŸºäºå®¢è§‚çš„éŸ³é¢‘æŠ€æœ¯æŒ‡æ ‡ï¼š
        1. éŸ³é¢‘ä¿çœŸåº¦ - ä¿¡å™ªæ¯”ã€å¤±çœŸåº¦
        2. æ··éŸ³è´¨é‡ - é¢‘è°±å¹³è¡¡ã€ç«‹ä½“å£°åƒ
        3. éŸ³è‰²è‡ªç„¶åº¦ - è°æ³¢ç»“æ„ã€å…±æŒ¯å³°
        """
        features = {}
        
        print("[æŠ€æœ¯è´¨é‡] åˆ†æéŸ³é¢‘ä¿çœŸåº¦...")
        
        # 1. éŸ³é¢‘ä¿çœŸåº¦æŒ‡æ ‡
        
        # ä¿¡å™ªæ¯”ä¼°ç®— - åŸºäºä¿¡å·å’Œå™ªå£°çš„èƒ½é‡æ¯”
        try:
            # ä½¿ç”¨é¢‘è°±é—¨é™æ³•ä¼°ç®—å™ªå£°
            stft = librosa.stft(y)
            magnitude = np.abs(stft)
            noise_threshold = np.percentile(magnitude, 10)  # åº•éƒ¨10%ä½œä¸ºå™ªå£°
            signal_energy = np.mean(magnitude[magnitude > noise_threshold] ** 2)
            noise_energy = np.mean(magnitude[magnitude <= noise_threshold] ** 2)
            snr_estimate = 10 * np.log10(signal_energy / (noise_energy + 1e-10))
            features['snr_estimate'] = np.clip(snr_estimate / 60, 0, 1)  # å½’ä¸€åŒ–åˆ°0-1
        except:
            features['snr_estimate'] = 0.5
        
        # æ€»è°æ³¢å¤±çœŸä¼°ç®— - åŸºäºè°æ³¢åˆ†æ
        try:
            # ç®€åŒ–çš„THDä¼°ç®—ï¼šéåŸºé¢‘èƒ½é‡å æ¯”
            pitches, magnitudes = librosa.piptrack(y=y, sr=self.sr)
            fundamental_energy = np.sum(magnitudes[magnitudes > np.percentile(magnitudes, 90)])
            total_energy = np.sum(magnitudes)
            thd_estimate = 1 - (fundamental_energy / (total_energy + 1e-10))
            features['thd_estimate'] = 1 - np.clip(thd_estimate, 0, 1)  # è¶Šå°è¶Šå¥½ï¼Œå–å
        except:
            features['thd_estimate'] = 0.5
        
        # 2. æ··éŸ³è´¨é‡æŒ‡æ ‡
        print("[æŠ€æœ¯è´¨é‡] åˆ†ææ··éŸ³è´¨é‡...")
        
        # é¢‘è°±å¹³è¡¡åº¦ - å„é¢‘æ®µèƒ½é‡åˆ†å¸ƒå‡åŒ€æ€§
        try:
            # å°†é¢‘è°±åˆ†ä¸ºä½ã€ä¸­ã€é«˜é¢‘ä¸‰æ®µ
            fft = np.fft.fft(y)
            freqs = np.fft.fftfreq(len(fft), 1/self.sr)
            magnitude_spectrum = np.abs(fft)
            
            # é¢‘æ®µåˆ’åˆ†ï¼šä½é¢‘0-500Hz, ä¸­é¢‘500-4000Hz, é«˜é¢‘4000Hz+
            low_freq_mask = (freqs >= 0) & (freqs <= 500)
            mid_freq_mask = (freqs > 500) & (freqs <= 4000)
            high_freq_mask = freqs > 4000
            
            low_energy = np.mean(magnitude_spectrum[low_freq_mask])
            mid_energy = np.mean(magnitude_spectrum[mid_freq_mask])
            high_energy = np.mean(magnitude_spectrum[high_freq_mask])
            
            # é¢‘è°±å¹³è¡¡åº¦ï¼šä¸‰ä¸ªé¢‘æ®µèƒ½é‡çš„æ ‡å‡†å·®è¶Šå°è¶Šå¹³è¡¡
            freq_balance = 1 - np.std([low_energy, mid_energy, high_energy]) / np.mean([low_energy, mid_energy, high_energy])
            features['frequency_balance'] = np.clip(freq_balance, 0, 1)
        except:
            features['frequency_balance'] = 0.5
        
        # åŠ¨æ€èŒƒå›´è´¨é‡
        rms = librosa.feature.rms(y=y)[0]
        dynamic_range_db = 20 * np.log10(np.max(rms) / (np.min(rms[rms > 0]) + 1e-10))
        features['dynamic_range_quality'] = np.clip(dynamic_range_db / 40, 0, 1)  # 40dBä¸ºæ»¡åˆ†
        
        # 3. éŸ³è‰²è‡ªç„¶åº¦æŒ‡æ ‡
        print("[æŠ€æœ¯è´¨é‡] åˆ†æéŸ³è‰²è‡ªç„¶åº¦...")
        
        # è°æ³¢ç»“æ„è‡ªç„¶åº¦ - åŸºäºHPSSåˆ†ç¦»
        try:
            harmonic, percussive = librosa.effects.hpss(y)
            harmonic_rms = librosa.feature.rms(y=harmonic)[0]
            percussive_rms = librosa.feature.rms(y=percussive)[0]
            
            # è°æ³¢-å†²å‡»å¹³è¡¡åº¦
            hp_balance = np.corrcoef(harmonic_rms, percussive_rms)[0, 1]
            features['harmonic_naturalness'] = (hp_balance + 1) / 2 if not np.isnan(hp_balance) else 0.5
        except:
            features['harmonic_naturalness'] = 0.5
        
        # é¢‘è°±ä¸€è‡´æ€§ - æ—¶é—´ç»´åº¦ä¸Šçš„é¢‘è°±ç¨³å®šæ€§
        try:
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=self.sr)[0]
            spectral_consistency = 1 - (np.std(spectral_centroids) / np.mean(spectral_centroids))
            features['spectral_consistency'] = np.clip(spectral_consistency, 0, 1)
        except:
            features['spectral_consistency'] = 0.5
        
        print(f"[æŠ€æœ¯è´¨é‡] ç‰¹å¾æå–å®Œæˆï¼Œå…±{len(features)}ä¸ªç‰¹å¾")
        return features
    
    def extract_artistic_quality_features(self, y: np.ndarray) -> Dict[str, float]:
        """æå–è‰ºæœ¯è´¨é‡ç‰¹å¾
        
        åŸºäºéŸ³ä¹ç†è®ºå’Œç¾å­¦åŸç†ï¼š
        1. æ—‹å¾‹åˆ›æ–°æ€§ - éŸ³ç¨‹åˆ†å¸ƒã€èŠ‚å¥å¤æ‚åº¦
        2. å’Œå£°å¤æ‚åº¦ - å’Œå¼¦è¿›è¡Œã€è°ƒæ€§åˆ†æ
        3. æƒ…æ„Ÿè¡¨è¾¾ - éŸ³è‰²å˜åŒ–ã€åŠ›åº¦å¯¹æ¯”
        """
        features = {}
        
        print("[è‰ºæœ¯è´¨é‡] åˆ†ææ—‹å¾‹åˆ›æ–°æ€§...")
        
        # 1. æ—‹å¾‹åˆ›æ–°æ€§æŒ‡æ ‡
        
        # éŸ³ç¨‹åˆ†å¸ƒç†µ - æ—‹å¾‹çš„éŸ³ç¨‹ä½¿ç”¨å¤šæ ·æ€§
        try:
            pitches, magnitudes = librosa.piptrack(y=y, sr=self.sr)
            pitch_sequence = []
            for t in range(pitches.shape[1]):
                index = magnitudes[:, t].argmax()
                if pitches[index, t] > 0:
                    pitch_sequence.append(pitches[index, t])
            
            if len(pitch_sequence) > 5:
                intervals = np.diff(pitch_sequence)
                interval_hist, _ = np.histogram(intervals, bins=20)
                interval_entropy = stats.entropy(interval_hist + 1e-10)
                features['melodic_innovation'] = min(interval_entropy / 3, 1.0)
            else:
                features['melodic_innovation'] = 0.5
        except:
            features['melodic_innovation'] = 0.5
        
        # èŠ‚å¥å¤æ‚åº¦ - åŸºäºonsetæ£€æµ‹
        try:
            onset_frames = librosa.onset.onset_detect(y=y, sr=self.sr, units='frames')
            if len(onset_frames) > 5:
                onset_intervals = np.diff(onset_frames)
                rhythm_complexity = stats.entropy(np.histogram(onset_intervals, bins=10)[0] + 1e-10)
                features['rhythmic_complexity'] = min(rhythm_complexity / 2.5, 1.0)
            else:
                features['rhythmic_complexity'] = 0.5
        except:
            features['rhythmic_complexity'] = 0.5
        
        # 2. å’Œå£°å¤æ‚åº¦æŒ‡æ ‡
        print("[è‰ºæœ¯è´¨é‡] åˆ†æå’Œå£°å¤æ‚åº¦...")
        
        # å’Œå¼¦è¿›è¡Œå¤æ‚åº¦ - åŸºäºå®‰å…¨çš„é¢‘è°±å˜åŒ–åˆ†æ
        try:
            # ä½¿ç”¨å®‰å…¨çš„é¢‘è°±åˆ†ææ›¿ä»£è‰²åº¦ç‰¹å¾
            y_short = y[:min(len(y), self.sr * 15)]  # é™åˆ¶åˆ°15ç§’
            if len(y_short) >= 4096:
                # åˆ†æ®µåˆ†æé¢‘è°±å˜åŒ–
                segment_length = 2048
                spectral_changes = []
                
                for i in range(0, len(y_short) - segment_length, segment_length // 2):
                    segment1 = y_short[i:i + segment_length]
                    segment2 = y_short[i + segment_length//2:i + segment_length//2 + segment_length]
                    
                    if len(segment1) == segment_length and len(segment2) == segment_length:
                        # è®¡ç®—ä¸¤æ®µçš„é¢‘è°±
                        fft1 = np.abs(np.fft.fft(segment1, n=2048)[:1024])
                        fft2 = np.abs(np.fft.fft(segment2, n=2048)[:1024])
                        
                        # è®¡ç®—é¢‘è°±å˜åŒ–
                        spectral_diff = np.mean(np.abs(fft2 - fft1))
                        spectral_changes.append(spectral_diff)
                
                if spectral_changes:
                    harmonic_motion = np.mean(spectral_changes)
                    features['harmonic_complexity'] = min(harmonic_motion / 1000, 1.0)
                else:
                    features['harmonic_complexity'] = 0.5
            else:
                features['harmonic_complexity'] = 0.5
        except:
            features['harmonic_complexity'] = 0.5
        
        # è°ƒæ€§ç¨³å®šæ€§åˆ†æ - åŸºäºé¢‘è°±èƒ½é‡åˆ†å¸ƒçš„ç¨³å®šæ€§
        try:
            # ä½¿ç”¨é¢‘è°±åˆ†ææ›¿ä»£è‰²åº¦åˆ†æ
            if len(y_short) >= 2048:
                # è®¡ç®—æ•´æ®µéŸ³é¢‘çš„é¢‘è°±åˆ†å¸ƒ
                fft_result = np.fft.fft(y_short, n=4096)
                magnitude = np.abs(fft_result[:2048])
                
                # å°†é¢‘è°±åˆ†ä¸º12ä¸ªåŒºé—´ï¼ˆæ¨¡æ‹ŸåäºŒå¹³å‡å¾‹ï¼‰
                freq_bins = len(magnitude)
                bin_size = max(1, freq_bins // 12)
                chroma_like = []
                
                for i in range(12):
                    start_idx = i * bin_size
                    end_idx = min((i + 1) * bin_size, freq_bins)
                    if start_idx < end_idx:
                        bin_energy = np.mean(magnitude[start_idx:end_idx])
                        chroma_like.append(bin_energy)
                
                if len(chroma_like) == 12:
                    chroma_array = np.array(chroma_like)
                    chroma_norm = chroma_array / (np.sum(chroma_array) + 1e-10)
                    
                    # è®¡ç®—ä¸å¤§è°ƒéŸ³é˜¶çš„ç›¸ä¼¼æ€§
                    major_scale = np.array([1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1])  # Cå¤§è°ƒ
                    correlations = []
                    for i in range(12):
                        shifted_scale = np.roll(major_scale, i)
                        corr = np.corrcoef(chroma_norm, shifted_scale)[0, 1]
                        if not np.isnan(corr):
                            correlations.append(corr)
                    
                    if correlations:
                        tonal_clarity = np.max(correlations)
                        features['tonal_stability'] = (tonal_clarity + 1) / 2
                    else:
                        features['tonal_stability'] = 0.5
                else:
                    features['tonal_stability'] = 0.5
            else:
                features['tonal_stability'] = 0.5
        except:
            features['tonal_stability'] = 0.5
        
        # 3. æƒ…æ„Ÿè¡¨è¾¾æŒ‡æ ‡
        print("[è‰ºæœ¯è´¨é‡] åˆ†ææƒ…æ„Ÿè¡¨è¾¾...")
        
        # éŸ³è‰²å˜åŒ–ä¸°å¯Œåº¦ - åŸºäºMFCCå˜åŒ–
        try:
            mfccs = librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=13)
            mfcc_variance = np.mean(np.var(mfccs, axis=1))
            features['timbral_variation'] = min(mfcc_variance / 10, 1.0)
        except:
            features['timbral_variation'] = 0.5
        
        # åŠ›åº¦å¯¹æ¯”åº¦ - åŸºäºRMSèƒ½é‡å˜åŒ–
        try:
            rms = librosa.feature.rms(y=y)[0]
            dynamic_contrast = (np.max(rms) - np.min(rms)) / (np.max(rms) + 1e-10)
            features['dynamic_expression'] = np.clip(dynamic_contrast, 0, 1)
        except:
            features['dynamic_expression'] = 0.5
        
        # æƒ…æ„Ÿä¸€è‡´æ€§ - åŸºäºé¢‘è°±è´¨å¿ƒè½¨è¿¹çš„å¹³æ»‘åº¦
        try:
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=self.sr)[0]
            emotional_consistency = np.corrcoef(spectral_centroids[:-1], spectral_centroids[1:])[0, 1]
            features['emotional_consistency'] = (emotional_consistency + 1) / 2 if not np.isnan(emotional_consistency) else 0.5
        except:
            features['emotional_consistency'] = 0.5
        
        print(f"[è‰ºæœ¯è´¨é‡] ç‰¹å¾æå–å®Œæˆï¼Œå…±{len(features)}ä¸ªç‰¹å¾")
        return features
    
    def extract_listening_experience_features(self, y: np.ndarray) -> Dict[str, float]:
        """æå–å¬è§‰ä½“éªŒç‰¹å¾
        
        åŸºäºå¿ƒç†å£°å­¦åŸç†ï¼š
        1. æ•´ä½“åè°ƒæ€§ - é¢‘è°±ç›¸å…³æ€§ã€æ—¶åŸŸä¸€è‡´æ€§
        2. åŠ¨æ€å˜åŒ– - å“åº¦å˜åŒ–ç‡ã€é¢‘è°±æ¼”åŒ–
        3. ç©ºé—´æ„Ÿ - ç«‹ä½“å£°å®½åº¦ã€æ·±åº¦æ„ŸçŸ¥
        """
        features = {}
        
        print("[å¬è§‰ä½“éªŒ] åˆ†ææ•´ä½“åè°ƒæ€§...")
        
        # 1. æ•´ä½“åè°ƒæ€§æŒ‡æ ‡
        
        # é¢‘è°±ç›¸å…³æ€§ - ä¸åŒæ—¶é—´æ®µé¢‘è°±çš„ç›¸ä¼¼æ€§
        try:
            stft = librosa.stft(y)
            magnitude = np.abs(stft)
            
            # è®¡ç®—ç›¸é‚»æ—¶é—´å¸§çš„é¢‘è°±ç›¸å…³æ€§
            correlations = []
            for i in range(magnitude.shape[1] - 1):
                corr = np.corrcoef(magnitude[:, i], magnitude[:, i + 1])[0, 1]
                if not np.isnan(corr):
                    correlations.append(corr)
            
            spectral_coherence = np.mean(correlations) if correlations else 0.5
            features['spectral_coherence'] = (spectral_coherence + 1) / 2
        except:
            features['spectral_coherence'] = 0.5
        
        # æ—¶åŸŸä¸€è‡´æ€§ - åŸºäºè‡ªç›¸å…³åˆ†æ
        try:
            # è®¡ç®—éŸ³é¢‘çš„è‡ªç›¸å…³å‡½æ•°
            autocorr = np.correlate(y, y, mode='full')
            autocorr = autocorr[len(autocorr)//2:]
            
            # å¯»æ‰¾ä¸»è¦çš„å‘¨æœŸæ€§æˆåˆ†
            peaks = signal.find_peaks(autocorr[:len(autocorr)//4])[0]
            if len(peaks) > 0:
                periodicity_strength = autocorr[peaks[0]] / autocorr[0] if autocorr[0] != 0 else 0
                features['temporal_consistency'] = np.clip(periodicity_strength, 0, 1)
            else:
                features['temporal_consistency'] = 0.5
        except:
            features['temporal_consistency'] = 0.5
        
        # 2. åŠ¨æ€å˜åŒ–æŒ‡æ ‡
        print("[å¬è§‰ä½“éªŒ] åˆ†æåŠ¨æ€å˜åŒ–...")
        
        # å“åº¦å˜åŒ–ç‡ - åŸºäºæ„ŸçŸ¥å“åº¦æ¨¡å‹
        try:
            # ä½¿ç”¨A-weightingè¿‘ä¼¼æ„ŸçŸ¥å“åº¦
            rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
            loudness_changes = np.abs(np.diff(rms))
            dynamic_activity = np.mean(loudness_changes) / (np.mean(rms) + 1e-10)
            features['dynamic_activity'] = min(dynamic_activity * 10, 1.0)
        except:
            features['dynamic_activity'] = 0.5
        
        # é¢‘è°±æ¼”åŒ–åº¦ - é¢‘è°±éšæ—¶é—´çš„å˜åŒ–ç¨‹åº¦
        try:
            spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=self.sr)[0]
            spectral_evolution = np.std(spectral_centroids) / np.mean(spectral_centroids)
            features['spectral_evolution'] = min(spectral_evolution * 5, 1.0)
        except:
            features['spectral_evolution'] = 0.5
        
        # 3. ç©ºé—´æ„ŸæŒ‡æ ‡ï¼ˆå¯¹å•å£°é“éŸ³é¢‘çš„è¿‘ä¼¼åˆ†æï¼‰
        print("[å¬è§‰ä½“éªŒ] åˆ†æç©ºé—´æ„Ÿ...")
        
        # é¢‘è°±å®½åº¦ - åŸºäºé¢‘è°±å¸¦å®½
        try:
            spectral_bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=self.sr)[0]
            spatial_width = np.mean(spectral_bandwidth) / (self.sr / 2)  # å½’ä¸€åŒ–
            features['spatial_width'] = np.clip(spatial_width, 0, 1)
        except:
            features['spatial_width'] = 0.5
        
        # æ·±åº¦æ„ŸçŸ¥ - åŸºäºæ··å“ä¼°ç®—
        try:
            # ç®€åŒ–çš„æ··å“æ£€æµ‹ï¼šåæœŸèƒ½é‡è¡°å‡åˆ†æ
            energy_envelope = librosa.feature.rms(y=y, frame_length=4096, hop_length=1024)[0]
            
            # å¯»æ‰¾èƒ½é‡å³°å€¼åçš„è¡°å‡æ¨¡å¼
            peaks = signal.find_peaks(energy_envelope, height=np.percentile(energy_envelope, 80))[0]
            
            if len(peaks) > 0:
                # åˆ†æç¬¬ä¸€ä¸ªå³°å€¼åçš„è¡°å‡
                peak_idx = peaks[0]
                if peak_idx < len(energy_envelope) - 10:
                    decay_segment = energy_envelope[peak_idx:peak_idx + 10]
                    decay_rate = np.polyfit(range(len(decay_segment)), decay_segment, 1)[0]
                    depth_perception = min(abs(decay_rate) * 100, 1.0)
                    features['depth_perception'] = depth_perception
                else:
                    features['depth_perception'] = 0.5
            else:
                features['depth_perception'] = 0.5
        except:
            features['depth_perception'] = 0.5
        
        # æ•´ä½“å¹³è¡¡æ„Ÿ - å„é¢‘æ®µèƒ½é‡çš„æ—¶é—´ç¨³å®šæ€§
        try:
            # åˆ†æä½ä¸­é«˜é¢‘çš„æ—¶é—´ç¨³å®šæ€§
            stft = librosa.stft(y)
            freqs = librosa.fft_frequencies(sr=self.sr)
            
            low_band = np.mean(np.abs(stft[freqs <= 500]), axis=0)
            mid_band = np.mean(np.abs(stft[(freqs > 500) & (freqs <= 4000)]), axis=0)
            high_band = np.mean(np.abs(stft[freqs > 4000]), axis=0)
            
            # è®¡ç®—å„é¢‘æ®µçš„ç¨³å®šæ€§
            low_stability = 1 - (np.std(low_band) / (np.mean(low_band) + 1e-10))
            mid_stability = 1 - (np.std(mid_band) / (np.mean(mid_band) + 1e-10))
            high_stability = 1 - (np.std(high_band) / (np.mean(high_band) + 1e-10))
            
            overall_balance = np.mean([low_stability, mid_stability, high_stability])
            features['overall_balance'] = np.clip(overall_balance, 0, 1)
        except:
            features['overall_balance'] = 0.5
        
        print(f"[å¬è§‰ä½“éªŒ] ç‰¹å¾æå–å®Œæˆï¼Œå…±{len(features)}ä¸ªç‰¹å¾")
        return features
    
    def _calculate_quantization_degree(self, pitch_sequence: List[float]) -> float:
        """è®¡ç®—éŸ³é«˜çš„é‡åŒ–ç¨‹åº¦
        
        AIç”Ÿæˆçš„éŸ³ä¹å¾€å¾€éŸ³é«˜è¿‡äºé‡åŒ–ï¼ˆæ¥è¿‘åŠéŸ³çš„æ•´æ•°å€ï¼‰
        äººç±»æ¼”å¥ä¼šæœ‰å¾®å¦™çš„éŸ³é«˜åç§»
        """
        if len(pitch_sequence) < 5:
            return 0.5
        
        # å°†éŸ³é«˜è½¬æ¢ä¸ºåŠéŸ³å•ä½
        semitones = 12 * np.log2(np.array(pitch_sequence) / 440.0) + 69  # A4 = 440Hz = 69åŠéŸ³
        
        # è®¡ç®—æ¯ä¸ªéŸ³é«˜ä¸æœ€è¿‘åŠéŸ³çš„åå·®
        deviations = np.abs(semitones - np.round(semitones))
        
        # é‡åŒ–ç¨‹åº¦ï¼šåå·®è¶Šå°ï¼Œé‡åŒ–ç¨‹åº¦è¶Šé«˜
        quantization_degree = 1 - np.mean(deviations) / 0.5  # 0.5åŠéŸ³ä¸ºæœ€å¤§åˆç†åå·®
        
        return np.clip(quantization_degree, 0, 1)
    
    def extract_comprehensive_features(self, audio_file: str) -> Dict[str, float]:
        """æå–éŸ³é¢‘çš„å…¨é¢ç‰¹å¾"""
        print(f"\n[ç»¼åˆç‰¹å¾æå–] å¼€å§‹å¤„ç†: {os.path.basename(audio_file)}")
        
        try:
            # åŠ è½½éŸ³é¢‘
            y, sr = librosa.load(audio_file, sr=self.sr)
            y = librosa.util.normalize(y)
            
            # æå–å„ç»´åº¦ç‰¹å¾
            ai_features = self.extract_ai_involvement_features(y)
            tech_features = self.extract_technical_quality_features(y)
            art_features = self.extract_artistic_quality_features(y)
            exp_features = self.extract_listening_experience_features(y)
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            all_features = {}
            all_features.update({f'ai_{k}': v for k, v in ai_features.items()})
            all_features.update({f'tech_{k}': v for k, v in tech_features.items()})
            all_features.update({f'art_{k}': v for k, v in art_features.items()})
            all_features.update({f'exp_{k}': v for k, v in exp_features.items()})
            
            print(f"[ç»¼åˆç‰¹å¾æå–] å®Œæˆï¼Œå…±æå–{len(all_features)}ä¸ªç‰¹å¾")
            return all_features
            
        except Exception as e:
            print(f"[é”™è¯¯] ç‰¹å¾æå–å¤±è´¥ {audio_file}: {e}")
            # è¿”å›é»˜è®¤ç‰¹å¾å€¼
            return {f'feature_{i}': 0.5 for i in range(20)}
    
    def calculate_dimension_scores(self, features: Dict[str, float]) -> Dict[str, float]:
        """æ ¹æ®ç‰¹å¾è®¡ç®—å„ç»´åº¦å¾—åˆ†"""
        scores = {}
        
        # 1. AIå‚ä¸åº¦å¾—åˆ† (0-20åˆ†)
        ai_features = [v for k, v in features.items() if k.startswith('ai_')]
        if ai_features:
            # AIå‚ä¸åº¦è¶Šé«˜ï¼Œå¾—åˆ†è¶Šé«˜ï¼ˆè¡¨ç¤ºæ›´å¤šäººå·¥å‚ä¸ï¼‰
            ai_involvement_raw = np.mean(ai_features)
            scores['ai_involvement'] = ai_involvement_raw * 20
        else:
            scores['ai_involvement'] = 10
        
        # 2. æŠ€æœ¯è´¨é‡å¾—åˆ† (0-35åˆ†)
        tech_features = [v for k, v in features.items() if k.startswith('tech_')]
        if tech_features:
            tech_quality_raw = np.mean(tech_features)
            scores['technical_quality'] = tech_quality_raw * 35
        else:
            scores['technical_quality'] = 17.5
        
        # 3. è‰ºæœ¯è´¨é‡å¾—åˆ† (0-30åˆ†)
        art_features = [v for k, v in features.items() if k.startswith('art_')]
        if art_features:
            art_quality_raw = np.mean(art_features)
            scores['artistic_quality'] = art_quality_raw * 30
        else:
            scores['artistic_quality'] = 15
        
        # 4. å¬è§‰ä½“éªŒå¾—åˆ† (0-15åˆ†)
        exp_features = [v for k, v in features.items() if k.startswith('exp_')]
        if exp_features:
            exp_quality_raw = np.mean(exp_features)
            scores['listening_experience'] = exp_quality_raw * 15
        else:
            scores['listening_experience'] = 7.5
        
        return scores
    
    def calculate_comprehensive_score(self, dimension_scores: Dict[str, float]) -> Tuple[float, str, Dict]:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        
        # åŠ æƒæ±‚å’Œ
        total_score = sum(dimension_scores[dim] * self.dimension_weights[dim] 
                         for dim in dimension_scores.keys())
        
        # ç¡®å®šç­‰çº§
        if total_score >= 90:
            grade = "ä¼˜ç§€"
            category = "human"
        elif total_score >= 80:
            grade = "è‰¯å¥½"
            category = "ai_assisted_high"
        elif total_score >= 70:
            grade = "ä¸­ç­‰"
            category = "ai_assisted_low"
        elif total_score >= 60:
            grade = "åŠæ ¼"
            category = "ai_direct_high"
        elif total_score >= 40:
            grade = "è¾ƒå·®"
            category = "ai_direct_low"
        else:
            grade = "å·®"
            category = "ai_generated_poor"
        
        # è¯¦ç»†åˆ†æ
        analysis = {
            'total_score': total_score,
            'grade': grade,
            'category': category,
            'dimension_scores': dimension_scores,
            'strengths': [],
            'weaknesses': [],
            'suggestions': []
        }
        
        # åˆ†æä¼˜ç¼ºç‚¹
        for dim, score in dimension_scores.items():
            max_score = {'ai_involvement': 20, 'technical_quality': 35, 
                        'artistic_quality': 30, 'listening_experience': 15}[dim]
            percentage = score / max_score
            
            if percentage >= 0.8:
                analysis['strengths'].append(f"{dim}: {score:.1f}/{max_score} (ä¼˜ç§€)")
            elif percentage <= 0.5:
                analysis['weaknesses'].append(f"{dim}: {score:.1f}/{max_score} (éœ€æ”¹è¿›)")
        
        # æ”¹è¿›å»ºè®®
        if dimension_scores['technical_quality'] / 35 < 0.6:
            analysis['suggestions'].append("æå‡æŠ€æœ¯åˆ¶ä½œæ°´å¹³ï¼šæ”¹å–„éŸ³é¢‘ä¿çœŸåº¦å’Œæ··éŸ³è´¨é‡")
        if dimension_scores['artistic_quality'] / 30 < 0.6:
            analysis['suggestions'].append("å¢å¼ºè‰ºæœ¯è¡¨ç°åŠ›ï¼šä¸°å¯Œæ—‹å¾‹åˆ›æ–°å’Œæƒ…æ„Ÿè¡¨è¾¾")
        if dimension_scores['ai_involvement'] / 20 < 0.4:
            analysis['suggestions'].append("å¢åŠ äººå·¥åˆ›ä½œå‚ä¸ï¼šå‡å°‘AIç”Ÿæˆç—•è¿¹ï¼Œå¢åŠ äººå·¥è°ƒæ ¡")
        
        return total_score, grade, analysis
    
    def evaluate_single_audio(self, audio_file: str) -> Dict:
        """è¯„ä¼°å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
        print(f"\n{'='*60}")
        print(f"å¼€å§‹è¯„ä¼°: {os.path.basename(audio_file)}")
        print(f"{'='*60}")
        
        # æå–ç‰¹å¾
        features = self.extract_comprehensive_features(audio_file)
        
        # è®¡ç®—ç»´åº¦å¾—åˆ†
        dimension_scores = self.calculate_dimension_scores(features)
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        total_score, grade, analysis = self.calculate_comprehensive_score(dimension_scores)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            'filename': os.path.basename(audio_file),
            'total_score': total_score,
            'grade': grade,
            'category': analysis['category'],
            'dimension_scores': dimension_scores,
            'detailed_analysis': analysis,
            'features': features
        }
        
        # æ‰“å°è¯„ä¼°ç»“æœ
        print(f"\nğŸµ éŸ³é¢‘æ–‡ä»¶: {report['filename']}")
        print(f"ğŸ“Š ç»¼åˆè¯„åˆ†: {total_score:.1f}/100")
        print(f"ğŸ“ˆ è¯„ä»·ç­‰çº§: {grade}")
        print(f"ğŸ·ï¸  éŸ³ä¹ç±»å‹: {analysis['category']}")
        print(f"\nğŸ“‹ è¯¦ç»†å¾—åˆ†:")
        for dim, score in dimension_scores.items():
            max_scores = {'ai_involvement': 20, 'technical_quality': 35, 
                         'artistic_quality': 30, 'listening_experience': 15}
            print(f"  â€¢ {dim}: {score:.1f}/{max_scores[dim]}")
        
        if analysis['strengths']:
            print(f"\nâœ… ä¸»è¦ä¼˜ç‚¹:")
            for strength in analysis['strengths']:
                print(f"  â€¢ {strength}")
        
        if analysis['weaknesses']:
            print(f"\nâš ï¸  ä¸»è¦ç¼ºç‚¹:")
            for weakness in analysis['weaknesses']:
                print(f"  â€¢ {weakness}")
        
        if analysis['suggestions']:
            print(f"\nğŸ’¡ æ”¹è¿›å»ºè®®:")
            for suggestion in analysis['suggestions']:
                print(f"  â€¢ {suggestion}")
        
        return report

if __name__ == "__main__":
    print("=== åŸºäºæ•°å­¦ç†è®ºçš„AIéŸ³ä¹è´¨é‡è¯„ä»·ç³»ç»Ÿ ===")
    print("è®¾è®¡ç†å¿µï¼š")
    print("1. å¤šç»´åº¦è¯„ä»·ï¼šAIå‚ä¸åº¦ã€æŠ€æœ¯è´¨é‡ã€è‰ºæœ¯è´¨é‡ã€å¬è§‰ä½“éªŒ")
    print("2. æ•°å­¦åŸºç¡€ï¼šä¿¡å·å¤„ç†ã€ç»Ÿè®¡å­¦ã€éŸ³ä¹å£°å­¦ã€å¿ƒç†å£°å­¦")
    print("3. å¯è§£é‡Šæ€§ï¼šæ¯ä¸ªè¯„åˆ†éƒ½æœ‰æ˜ç¡®çš„æ•°å­¦å’ŒéŸ³ä¹ç†è®ºä¾æ®")
    print("4. å…¬å¹³æ€§ï¼šåŒºåˆ†AIè¾…åŠ©ç”Ÿæˆå’ŒAIç›´æ¥ç”Ÿæˆï¼Œé¿å…ä¸€åˆ€åˆ‡")
    print()
    
    # åˆ›å»ºè¯„ä»·å™¨å®ä¾‹
    evaluator = AIMusicQualityEvaluator()
    
    # ç¤ºä¾‹ï¼šè¯„ä¼°å•ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨çš„è¯ï¼‰
    test_files = glob.glob("é™„ä»¶å…­ï¼šè®­ç»ƒæ•°æ®/1-*")[:3]  # æµ‹è¯•å‰3ä¸ªæ–‡ä»¶
    
    if test_files:
        print(f"ğŸ” æ‰¾åˆ° {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶ï¼Œå¼€å§‹è¯„ä¼°...")
        
        all_reports = []
        for audio_file in test_files:
            try:
                report = evaluator.evaluate_single_audio(audio_file)
                all_reports.append(report)
            except Exception as e:
                print(f"âŒ è¯„ä¼°å¤±è´¥ {audio_file}: {e}")
        
        # ç»Ÿè®¡åˆ†æ
        if all_reports:
            print(f"\n{'='*60}")
            print("ğŸ“Š è¯„ä¼°ç»Ÿè®¡åˆ†æ")
            print(f"{'='*60}")
            
            scores = [r['total_score'] for r in all_reports]
            print(f"å¹³å‡åˆ†: {np.mean(scores):.1f}")
            print(f"æœ€é«˜åˆ†: {np.max(scores):.1f}")
            print(f"æœ€ä½åˆ†: {np.min(scores):.1f}")
            print(f"æ ‡å‡†å·®: {np.std(scores):.1f}")
            
            # ç­‰çº§åˆ†å¸ƒ
            grades = [r['grade'] for r in all_reports]
            from collections import Counter
            grade_dist = Counter(grades)
            print(f"\nç­‰çº§åˆ†å¸ƒ:")
            for grade, count in grade_dist.items():
                print(f"  {grade}: {count} ä¸ª")
    
    else:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•éŸ³é¢‘æ–‡ä»¶")
        print("è¯·ç¡®ä¿è®­ç»ƒæ•°æ®ç›®å½•å­˜åœ¨ä¸”åŒ…å«éŸ³é¢‘æ–‡ä»¶")
    
    print(f"\nâœ… AIéŸ³ä¹è´¨é‡è¯„ä»·ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")
