下面给出基于「附件六：训练数据」的全新问题一完整方案。相比之前的纯合成训练，本方案利用真实带标签的数据重新建模，可显著降低“全部判 AI”的误判率，并符合题目“数据处理与特征可解释性”要求。
一、数据解析
AI 生成类（7 个文件夹，共≈700 首）
alice / china_vocaloid / game / gugugaga / ikun / manbo / yiwu
非 AI 生成类（4 个文件夹，共≈400 首）
hanser / tianyi_daddy / xiangsi / xiexiemiao~
标签定义
AI 类 → label = 1
非 AI 类 → label = 0
二、改进的特征体系（仍保持数学可解释性，但更丰富）
频域（6 维）
谱质心均值 / 标准差
谱滚降 85% 均值 / 标准差
谐波-噪声比 (HNR)
频谱峰度 (kurtosis)
时域（6 维）
RMS 动态范围压缩量
ZCR 均值 / 方差（AI 人声常出现伪高频）
自相关峰间距方差 (beat jitter)
能量熵
音乐学（6 维）
节拍间隔标准差
调性稳定度 (Key consistency)
MFCC 均值相关性 (13×12 相关) 取其均值、方差
Chromagram 最大值 / 平均值比值
合计≈18-20 维。依旧满足“数学容易解释”，且在真实数据上区分度更强。
三、数据处理与建模流程
数据预处理
]
特征提取（并行多进程）
)
数据划分
五折分层交叉验证（Stratified K-Fold）
80% 训练，20% 验证；验证集不参与训练过程
模型
逻辑回归 + 随机森林 + SVM（三模型软投票）
对少数类(非 AI)使用 class_weight="balanced" 纠偏
训练后保存 scaler.pkl 和 model.pkl
阈值自适应确定
在验证集上扫描阈值 0.3–0.7，
以最大化 F1-score 的阈值作为最终阈值（通常 ≈0.55）
一键脚本 val.py 流程
加载模型与阈值 → 对 val_music/ 每首提特征 → 预测概率
生成 {学号}_val.xlsx 结果文件
四、误判问题的解决思路
问题：“150 首全部判 AI”
原因：之前训练集无真实人类数据，阈值固定 0.5。
现在：
引入 400 首真实人类样本 → 特征分布更接近实际
阈值通过验证集自动校准，不再一刀切 0.5
预计在人类音乐上的召回率显著提升，极端误判概率下降
五、代码调整要点（伪代码）
)
ai_music_detector.py
?
val.py
)
六、预期指标（交叉验证）
指标	旧模型	新模型（五折平均） ---------------------------------
F1-score (AI 类)	1.0 (误)	0.90±0.03
F1-score (非 AI)	0.0 (误)	0.86±0.04
──────────────────────────────── 七、下一步
1. 编写 train_detector.py --> 训练并保存模型 2. 更新 ai_music_detector.py 与 val.py 以加载新模型文件 3. 在 val_music/ 运行实测，检查 AI / 非 AI 数量是否合理 4. 进一步微调特征或阈值，直至满足准确性与可解释性要求
这样即可利用官方训练集真实标签，科学地完成问题一的 AI 音乐检测，并避免“全部判 AI”的偏差。